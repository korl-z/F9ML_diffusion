#training_conifg.yaml
training_config:
  max_epochs: 194
  early_stop_patience: 150

  ema_halflife_kimg: 500
  ema_rampup_ratio: 0.09
  std: 0.10  #use with PFEMA

  compile: False  
  optimizer:
    name: Adam
    lr: 1e-5
    weight_decay: 1e-5

  scheduler:
    # use: false
    use: true
    # scheduler_name: ReduceLROnPlateau
    # scheduler_params:
    #   mode: min
    #   factor: 0.5
    #   patience: 3
    #   verbose: true
    #   threshold: 1e-4
    # use: true
    # scheduler_name: CosineAnnealingLR
    # scheduler_params:
    #   T_max: 20
    #   eta_min: 0.0
    #   last_epoch: -1
    #   verbose: true
    scheduler_name: CosineAnnealingWarmRestarts
    interval: step 
    reduce_lr_on_epoch: 0.999
    scheduler_params:
      T_0: 6320
      T_mult: 2
      eta_min: 0

  # custom inv sqrt scheduler 1/sqrt(max(t/t_ref, 1))
    # scheduler_name: InverseSqrtLR
    # scheduler_params:
    #   t_ref: 70
    #   last_epoch: -1

  monitor: val_epoch_loss
  interval: epoch
